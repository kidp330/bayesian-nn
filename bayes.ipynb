{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesowskie sieci neuronowe\n",
    "\n",
    "Na dzisiejszych zajęciach opowiem czym są bayesowskie sieci neuronowe, jak wygląda podejście bayesowskie do głębokiego uczenia i jakimi różnicami charakteryzują się modele bayesowskie. \n",
    "\n",
    "Klasyczne uczenie modelu możemy interpretować jako przeprowadzanie Maximum Likelihood Estimation, tj. maksymalizujemy prawdopodobieństwo zaobserwowanych danych uwarunkowane parametrami modelu. Jest to podejście frekwentystyczne. \n",
    "Analogicznie w podejściu bayesowskim mówimy o Maximum A Posteriori - mamy jakiś prior, czyli jakąś poprzednią wiedzę, oraz obserwacje. Dzięki twierdzeniu Bayesa możemy uaktualnić nasze przekonania nowymi obserwacjami - otrzymujemy wtedy rozkład posterior. \n",
    "\n",
    "_Z perspektywy zatwardziałego statystyka używając jakichkolwiek technik regularyzacji przeprowadzamy MAP, a nie MLE, ponieważ dokładamy już jakąś wiedzę którą mamy o rozkładzie._\n",
    "\n",
    "Inne źródła opisują uczenie bayesowskie inaczej, np. że mamy po prostu modele których wagi są rozkładami prawdopodobieństwa, w przeciwieństwie do ustalonych wartości (point estimate). Istnieją algorytmy typu Bayes-By-Backprop, które faktycznie pozwalają na wytrenowanie sieci w sposób bardzo zbliżony do tego normalnego, ale nie są jedynymi na które warto zwrócić uwagę, lepiej przyjąć perspektywę uczenia się rozkładu w przestrzeni parametrów, aniżeli jednego optymalnego wektora.\n",
    "\n",
    "Jak to wygląda w praktyce? Co ciekawe, ucząc sieć bayesowską nie musi być wcale mowy o treningu. Tak jak normalnie, musimy najpierw dobrać odpowiednią do zadania architekturę (functional model w [2]), a poza tym również model stochastyczny, czyli priory. Ten krok można uważać za równoważny dobraniu lossa dla normalnej sieci.\n",
    "\n",
    "Oznaczmy $\\theta$ jako wektor parametrów. Jako priory będziemy musieli dobrać $p(\\theta)$ oraz $p(y | x, \\theta)$ - bazowy rozkład na przestrzeni parametrów i poziom pewności, że dla obserwacji $x$ i modelu $\\theta$ wylądujemy w klasie $y$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.convert_parameters import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_draw_plots(X, y, predicted):\n",
    "    fig = plt.figure(figsize = (16, 5))\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "    z1_plot = ax1.scatter(X[:, 0], X[:, 1], c = y)\n",
    "    z2_plot = ax2.scatter(X[:, 0], X[:, 1], c = predicted)\n",
    "\n",
    "    plt.colorbar(z1_plot,ax=ax1)\n",
    "    plt.colorbar(z2_plot,ax=ax2)\n",
    "\n",
    "    ax1.set_title(\"REAL\")\n",
    "    ax2.set_title(\"PREDICTED\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, shuffle=True, random_state=42)\n",
    "X_train, X_test = map(lambda t: torch.from_numpy(t).float(), [X_train, X_test])\n",
    "y_train, y_test = map(lambda t: torch.from_numpy(t).long(), [y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular = nn.Sequential(\n",
    "    nn.Linear(4, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 3),\n",
    "    nn.LogSoftmax(dim=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(regular.parameters(), lr=0.01)\n",
    "\n",
    "for _step in range(5):\n",
    "    pre = regular(X_train)\n",
    "    loss = F.nll_loss(pre, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    pre = regular(X_test)\n",
    "    _, predicted = torch.max(pre.data, 1)\n",
    "    total = y_test.size(0)\n",
    "    correct = (predicted == y_test).sum()\n",
    "    print('- Accuracy: %f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BayesLinear(nn.Module):\n",
    "    def __init__(self, prior_mu, prior_sigma, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.prior_mu = torch.tensor(prior_mu)\n",
    "        self.prior_sigma = torch.tensor(prior_sigma)\n",
    "        self.prior_log_sigma = torch.log(self.prior_sigma)\n",
    "        \n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_log_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_eps = torch.randn_like(self.weight_log_sigma)\n",
    "                \n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_log_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_eps = torch.randn_like(self.bias_log_sigma)\n",
    "                        \n",
    "        self.unfreeze()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialization method of Adv-BNN\n",
    "        stdv = 1. / torch.sqrt(torch.tensor(self.weight_mu.size(1)))\n",
    "        stdv = stdv.item()\n",
    "        self.weight_mu.data.uniform_(-stdv, stdv)\n",
    "        self.weight_log_sigma.data.fill_(self.prior_log_sigma)\n",
    "        self.bias_mu.data.uniform_(-stdv, stdv)\n",
    "        self.bias_log_sigma.data.fill_(self.prior_log_sigma)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        if not self._freeze:\n",
    "            # resample parameters\n",
    "            self.weight_eps = torch.randn_like(self.weight_log_sigma)\n",
    "            self.bias_eps = torch.randn_like(self.bias_log_sigma)\n",
    "\n",
    "        weight = self.weight_mu + torch.exp(self.weight_log_sigma) * self.weight_eps\n",
    "        bias = self.bias_mu + torch.exp(self.bias_log_sigma) * self.bias_eps\n",
    "\n",
    "        return F.linear(input, weight, bias)\n",
    "    \n",
    "    def freeze(self):\n",
    "        self._freeze = True\n",
    "    \n",
    "    def unfreeze(self):\n",
    "        self._freeze = False\n",
    "\n",
    "    def _kl_loss(self, mu_0, log_sigma_0, mu_1, log_sigma_1):\n",
    "        kl = log_sigma_1 - log_sigma_0 + \\\n",
    "            (torch.exp(log_sigma_0)**2 + (mu_0-mu_1)**2)/(2*torch.exp(log_sigma_1)**2) - 0.5\n",
    "        return kl.sum()\n",
    "\n",
    "    def kl_loss(self):\n",
    "        return self._kl_loss(self.weight_mu, self.weight_log_sigma, self.prior_mu, self.prior_log_sigma) + \\\n",
    "               self._kl_loss(self.bias_mu, self.bias_log_sigma, self.prior_mu, self.prior_log_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 1\n",
    "Napisz funkcję, która obliczy średnią dywergencję KL dowolnie głębokiej sieci korzystającej z warstw `BayesLinear`. Chodzi tu o zsumowanie wartości funkcji `kl_loss` dla tych warstw, i podzielenie przez sumaryczną liczbę parametrów (parę (mu, log_sigma) traktujemy jako jeden parametr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Zadanie 1 ###\n",
    "def kl_loss(model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian = nn.Sequential(\n",
    "    BayesLinear(0, 0.1, 4, 100),\n",
    "    nn.ReLU(),\n",
    "    BayesLinear(0, 0.1, 100, 3),\n",
    "    nn.LogSoftmax(dim=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(bayesian.parameters(), lr=0.01)\n",
    "kl_weight = 0.01\n",
    "\n",
    "for _step in range(50):\n",
    "    pre = bayesian(X_train)\n",
    "    _kl_loss = kl_loss(bayesian)\n",
    "    nll_loss = F.nll_loss(pre, y_train)\n",
    "    loss = nll_loss + kl_weight * _kl_loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    \n",
    "    print(\n",
    "        f'{_kl_loss.item()=:.6f}',\n",
    "        f'{nll_loss.item()=:.6f}',\n",
    "        f'{loss.item()=:.6f}',\n",
    "    sep=' | ')\n",
    "\n",
    "    pre = bayesian(X_test)\n",
    "    _, predicted = torch.max(pre.data, 1)\n",
    "    total = y_test.size(0)\n",
    "    correct = (predicted == y_test).sum()\n",
    "    print('- Accuracy: %f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = nn.Sequential(\n",
    "  BayesLinear(0, 0.1, 1, 100),\n",
    "  nn.ReLU(),\n",
    "  BayesLinear(0, 0.1, 100, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reg = torch.linspace(-3, 3, steps=500)\n",
    "y_reg = torch.sin(X_reg)\n",
    "y_reg += torch.randn_like(y_reg) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model):\n",
    "    xlims = (-3.4, 3.4)\n",
    "    X_test = torch.linspace(*xlims, 500 + 200)\n",
    "\n",
    "    predictions = torch.empty((10, *X_test.shape))\n",
    "    for i in range(10):\n",
    "        with torch.no_grad():\n",
    "            predictions[i] = model(X_test.unsqueeze(1)).squeeze()\n",
    "\n",
    "    std, mean = torch.std_mean(predictions, dim=0)\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    plt.xlim(xlims)\n",
    "    plt.ylim([-2, 2])\n",
    "    ax.plot(X_reg, y_reg, 'ko', markersize=4, label=\"Noisy training data\")\n",
    "    for p in predictions:\n",
    "        ax.plot(X_test, p, '-', color=\"purple\", linewidth=2, alpha=0.4)\n",
    "    ax.plot(X_test, mean, '-', color=\"#408765\", linewidth=5, label=\"Predictions\")\n",
    "    ax.fill_between(X_test, mean - 3 * std, mean + 3 * std, alpha=0.4, color='#86CFAC', zorder=5)\n",
    "\n",
    "    plt.legend(loc=4, fontsize=15, frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(regression_model.parameters(), lr=0.01)\n",
    "mse = nn.MSELoss()\n",
    "best_model = regression_model\n",
    "best_loss = float('inf')\n",
    "\n",
    "for step in range(1000):\n",
    "    pre = regression_model(X_reg.unsqueeze(1)).squeeze()\n",
    "    _kl_loss = kl_loss(regression_model)\n",
    "    kl_weight = 1 / (step + 1)\n",
    "    mse_loss = mse(pre, y_train)\n",
    "    loss = mse_loss + kl_weight * _kl_loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        optimizer.step()\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = deepcopy(regression_model)\n",
    "\n",
    "\n",
    "    if step % 250 == 0:\n",
    "        with torch.no_grad():\n",
    "            print(\n",
    "                f'{_kl_loss.item()=:.6f}',\n",
    "                f'{mse_loss.item()=:.6f}',\n",
    "                f'{best_loss.item()=:.6f}',\n",
    "            sep=' | ')\n",
    "\n",
    "        plot_predictions(best_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2\n",
    "Jakie wyniki osiągniemy uśredniając nie wyjście, a parametry modelu? Zapisz wnioski i obserwacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "\n",
    "ensemble = []\n",
    "predictions = torch.empty((10, *y_train.shape))\n",
    "for i in range(10):\n",
    "  with torch.no_grad():\n",
    "    ensemble.append(deepcopy(best_model.state_dict()))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Zadanie 2 ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
